// Auto-generated by expo-ondevice-ai config plugin
// This file is compiled with C++ interop enabled, isolated from React Native headers.

import Foundation
import Locanara
import LocalLLMClient
import LocalLLMClientLlama
import os.log
#if os(iOS)
import UIKit
#endif

private let logger = Logger(subsystem: "com.locanara.bridge", category: "LlamaCppBridge")

// MARK: - Bridge Engine (InferenceEngine conformance)

@available(iOS 17.0, *)
final class BridgedLlamaCppEngine: @unchecked Sendable, InferenceEngine, LlamaCppEngineProtocol {

    static var engineType: InferenceEngineType { .llamaCpp }
    var engineName: String { "On-Device LLM (llama.cpp via bridge)" }
    private(set) var isLoaded: Bool = false
    var isMultimodal: Bool { mmprojPath != nil }

    private var llmSession: LLMSession?
    private let modelPath: URL
    private let mmprojPath: URL?
    private var isCancelled = false
    private var isInferencing = false
    private let lock = NSLock()

    init(modelPath: URL, mmprojPath: URL?) {
        self.modelPath = modelPath
        self.mmprojPath = mmprojPath
    }

    private func beginInference() async throws {
        while lock.withLock({ isInferencing }) {
            try await Task.sleep(nanoseconds: 100_000_000)
        }
        lock.withLock {
            isInferencing = true
            isCancelled = false
        }
    }

    private func endInference() {
        lock.withLock { isInferencing = false }
    }

    func loadModel() async throws {
        guard !isLoaded else { return }

        guard FileManager.default.fileExists(atPath: modelPath.path) else {
            throw LocanaraError.modelNotDownloaded(modelPath.lastPathComponent)
        }

        let fileSize = (try? FileManager.default.attributesOfItem(atPath: modelPath.path)[.size] as? Int64) ?? 0
        guard fileSize >= 10_000_000 else {
            throw LocanaraError.modelLoadFailed("Invalid model file: too small")
        }

        let numThreads = max(4, ProcessInfo.processInfo.activeProcessorCount - 2)
        let llamaParam = LlamaClient.Parameter(
            context: 8192,
            seed: nil,
            numberOfThreads: numThreads,
            batch: 512,
            temperature: 0.5,
            topK: 40,
            topP: 0.9,
            typicalP: 1.0,
            penaltyLastN: 64,
            penaltyRepeat: 1.2,
            options: LlamaClient.Options(
                extraEOSTokens: ["</s>", "<end_of_turn>"],
                verbose: false
            )
        )

        let localModel = LLMSession.LocalModel.llama(
            url: modelPath,
            mmprojURL: mmprojPath,
            parameter: llamaParam
        )
        llmSession = LLMSession(model: localModel)
        try await llmSession?.prewarm()
        isLoaded = true
        logger.info("Bridge engine loaded model: \(self.modelPath.lastPathComponent)")
    }

    func generate(prompt: String, config: InferenceConfig) async throws -> String {
        try await beginInference()
        defer { endInference() }

        guard isLoaded, let session = llmSession else {
            throw LocanaraError.custom(.modelNotLoaded, "Model not loaded")
        }

        do {
            var result = try await session.respond(to: prompt)

            if let stops = config.stopSequences {
                for stop in stops {
                    if let range = result.range(of: stop) {
                        result = String(result[..<range.lowerBound])
                        break
                    }
                }
            }

            let maxChars = config.maxTokens * 4
            if result.count > maxChars {
                let truncated = String(result.prefix(maxChars))
                if let period = truncated.lastIndex(of: ".") {
                    result = String(truncated[...period])
                } else {
                    result = truncated
                }
            }

            return result.trimmingCharacters(in: .whitespacesAndNewlines)
        } catch let error as NSError {
            if error.domain == "LLMSession" || error.code == -1 {
                lock.withLock { isLoaded = false; llmSession = nil }
            }
            throw LocanaraError.executionFailed(error.localizedDescription)
        } catch {
            throw LocanaraError.executionFailed(error.localizedDescription)
        }
    }

    func generateStreaming(prompt: String, config: InferenceConfig) -> AsyncThrowingStream<String, Error> {
        AsyncThrowingStream { continuation in
            Task { [weak self] in
                guard let self else {
                    continuation.finish(throwing: LocanaraError.custom(.modelNotLoaded, "Model not loaded"))
                    return
                }
                do {
                    try await self.beginInference()
                } catch {
                    continuation.finish(throwing: error)
                    return
                }
                guard self.isLoaded, let session = self.llmSession else {
                    self.endInference()
                    continuation.finish(throwing: LocanaraError.custom(.modelNotLoaded, "Model not loaded"))
                    return
                }
                do {
                    for try await text in session.streamResponse(to: prompt) {
                        if self.lock.withLock({ self.isCancelled }) { break }
                        continuation.yield(text)
                    }
                    self.endInference()
                    continuation.finish()
                } catch {
                    self.endInference()
                    continuation.finish(throwing: LocanaraError.executionFailed(error.localizedDescription))
                }
            }
        }
    }

    func generateWithImage(prompt: String, imageData: Data, config: InferenceConfig) async throws -> String {
        guard isMultimodal else {
            throw LocanaraError.custom(.featureNotSupported, "mmproj file required for image input")
        }
        try await beginInference()
        defer { endInference() }

        guard isLoaded, let session = llmSession else {
            throw LocanaraError.custom(.modelNotLoaded, "Model not loaded")
        }

        #if os(iOS)
        guard let image = UIImage(data: imageData) else {
            throw LocanaraError.custom(.invalidInput, "Failed to create image from data")
        }
        let attachment = LLMAttachment.image(image)
        let response = try await session.respond(to: prompt, attachments: [attachment])
        return response.trimmingCharacters(in: .whitespacesAndNewlines)
        #else
        throw LocanaraError.custom(.featureNotSupported, "Image input not supported on this platform")
        #endif
    }

    func cancel() -> Bool {
        lock.lock()
        defer { lock.unlock() }
        if !isCancelled { isCancelled = true; return true }
        return false
    }

    func unload() {
        lock.lock()
        llmSession = nil
        isLoaded = false
        isInferencing = false
        lock.unlock()
        logger.info("Bridge engine unloaded")
    }
}

// MARK: - Bridge Provider (@objc discoverable by Locanara SDK)

@objc
@available(iOS 17.0, *)
public class LlamaCppBridgeEngine: NSObject, LlamaCppBridgeProvider {

    private var engine: BridgedLlamaCppEngine?
    private var isLoading = false
    private let loadLock = NSLock()

    public var isModelLoaded: Bool {
        engine?.isLoaded ?? false
    }

    public func loadAndRegisterModel(_ modelPath: String, mmprojPath: String?, completion: @escaping (NSError?) -> Void) {
        loadLock.lock()
        guard !isLoading else {
            loadLock.unlock()
            completion(NSError(domain: "LlamaCppBridge", code: -1, userInfo: [NSLocalizedDescriptionKey: "Model load already in progress"]))
            return
        }
        isLoading = true
        loadLock.unlock()

        Task {
            do {
                // Unload previous engine if any
                if let oldEngine = self.engine {
                    oldEngine.unload()
                    InferenceRouter.shared.unregisterEngine()
                }

                let modelURL = URL(fileURLWithPath: modelPath)
                let mmprojURL = mmprojPath.map { URL(fileURLWithPath: $0) }

                let newEngine = BridgedLlamaCppEngine(modelPath: modelURL, mmprojPath: mmprojURL)
                try await newEngine.loadModel()

                self.engine = newEngine
                InferenceRouter.shared.registerEngine(newEngine as any InferenceEngine)

                logger.info("Bridge: model loaded and engine registered")
                self.loadLock.withLock { self.isLoading = false }
                completion(nil)
            } catch {
                logger.error("Bridge: loadModel failed: \(error.localizedDescription)")
                self.loadLock.withLock { self.isLoading = false }
                completion(error as NSError)
            }
        }
    }

    public func unloadModel() {
        engine?.unload()
        InferenceRouter.shared.unregisterEngine()
        engine = nil
        logger.info("Bridge: model unloaded and engine unregistered")
    }
}
